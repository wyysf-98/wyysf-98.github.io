<!DOCTYPE html>

<head>
    <meta charset="utf-8" />
    <title>Example-based Motion Synthesis via Generative Motion Matching</title>
	<link rel="icon" type="image/x-icon" href="../assets/css/images/favicon.ico">
    <meta content="Example-based Motion Synthesis via Generative Motion Matching" name="description" />
    <meta content="summary" name="twitter:card" />
    <meta content="width=device-width, initial-scale=1" name="viewport" />
    <link href="static/css/template.css" rel="stylesheet" type="text/css" />
    <link href="static/css/my_style.css" rel="stylesheet" type="text/css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    
    <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
    <script type="text/javascript">
        WebFont.load({
            google: {
                families: ["Lato:100,100italic,300,300italic,400,400italic,700,700italic,900,900italic", "Montserrat:100,100italic,200,200italic,300,300italic,400,400italic,500,500italic,600,600italic,700,700italic,800,800italic,900,900italic", "Ubuntu:300,300italic,400,400italic,500,500italic,700,700italic", "Changa One:400,400italic", "Open Sans:300,300italic,400,400italic,600,600italic,700,700italic,800,800italic", "Varela Round:400", "Bungee Shade:regular", "Roboto:300,regular,500"]
            }
        });
    </script>
    <script type="text/javascript">
        ! function (o, c) {
            var n = c.documentElement,
                t = " w-mod-";
            n.className += t + "js", ("ontouchstart" in o || o.DocumentTouch && c instanceof DocumentTouch) && (n.className += t + "touch")
        }(window, document);
    </script>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <script type="text/javascript" src="static/js/zoom.js"></script>
    <script type="text/javascript" src="static/js/video_comparison.js"></script>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MLDP9MKGC8"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-MLDP9MKGC8');
    </script>
</head>

<body>
    <div class="section hero nerf-_v2">
        <div class="container-2 nerf_header_v2 w-container">
            <h1 class="nerf_title_v2">Example-based Motion Synthesis via <br> Generative Motion Matching</h1>
            <div class="nerf_subheader_v2">ACM Transactions on Graphics (SIGGRAPH 2023)</div>
            <div class="nerf_subheader_v2"><b style="color: rgb(230, 86, 29);">*still under construction</b></div>
            <div class="nerf_subheader_v2">
                <div>
                    <a href="https://wyysf-98.github.io/" target="_blank" class="nerf_authors_v2">Weiyu Li<span
                            class="text-span_nerf"></span></a><sup>*1</sup>,&nbsp;&nbsp;
                    <a href="https://xuelin-chen.github.io/" target="_blank" class="nerf_authors_v2">Xuelin Chen<span
                            class="text-span_nerf"></span></a><sup>*â€ 2</sup>,&nbsp;&nbsp;
                    <a href="https://peizhuoli.github.io/" target="_blank" class="nerf_authors_v2">Peizhuo Li<span
                            class="text-span_nerf"></span></a><sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://igl.ethz.ch/people/sorkine/" target="_blank" class="nerf_authors_v2">Olga Sorkine-Hornung<span
                            class="text-span_nerf"></span></a><sup>3</sup>,&nbsp;&nbsp;
                    <a href="https://cfcs.pku.edu.cn/baoquan/" target="_blank" class="nerf_authors_v2">Baoquan Chen<span
                            class="text-span_nerf"></span></a><sup>4</sup>
                </div>
                <div>
                    <h1 class="nerf_affiliation_v2"><sup>1</sup>Shandong University</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>2</sup>Tencent AI Lab</h1>,
                    <h1 class="nerf_affiliation_v2"><sup>3</sup>ETH Zurich</h1>
                    <h1 class="nerf_affiliation_v2"><sup>4</sup>Peking University</h1>
                </div>

                <div class="external-link">
                    <a class="btn" href="https://arxiv.org/abs/2306.00378" role="button" target="_blank">
                        <i class="ai ai-arxiv"></i> Arxiv </a>
                    <a class="btn" href="paper/Paper_high_res.pdf" role="button" target="_blank">
                        <i class="fa fa-file-pdf"></i> Paper </a>
                    <a class="btn" href="https://wyysf-98.github.io/GenMM_demo/" role="button" target="_blank">
                        <i class="fas fa-democrat"></i> Demo </a>
                    <a class="btn" href="https://github.com/wyysf-98/GenMM" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-github"></i> Code </a>
                    <a class="btn btn-large btn-light" href="https://youtu.be/lehnxcade4I" role="button" target="_blank" disabled>
                        <i class="fa-brands fa-youtube"></i> Video </a>
                </div>

            </div>
            <div>
                <h2 style="overflow: hidden">
                    <embed src="https://wyysf-98.github.io/GenMM_demo/" width="100%" height="500">
                </h2>
                <p align="left">
                    1. FBX parser in Three.js is not robust and lacks foot contact fix in web demo. Wait for Blender plugin for better performance.
                    <br>
                    2. The demo is powered by ðŸ¤—<a href="https://huggingface.co/spaces/wyysf/GenMM" class="nerf_authors_v2">Hugging Face</a>, 
                    which requires some extra time for data transmission (10s). 
                    <br>
                    3. Click 'Next' to try more examples. Have fun :)
                </p>
                <!-- <span class="center"><img src="assets/images/teaser.png"></span> -->
                <!-- <video class="video" id="Diverse-Generation" poster="assets/images/teaser.png" loop playsinline autoPlay
                    muted src="https://youtu.be/lehnxcade4I"></video> -->
                <!-- <div style="padding-top:56.17021276595745%" id="w-node-e5e45b1d55ac-81500a5f" class="video">
                    <iframe src="https://www.youtube.com/embed/lehnxcade4I?rel=1&amp;controls=1&amp;autoplay=0&amp;mute=0&amp;start=0" frameBorder="0" style="position:absolute;left:0;top:0;width:100%;height:100%;pointer-events:auto" allow="autoplay; encrypted-media" allowfullscreen=""></iframe>
                </div> -->
            </div>
        </div>

    </div>


    <div data-anchor="slide1" class="section nerf_section">
        <div class="w-container grey_container">
            <h2 class="grey-heading_nerf">Abstract</h2>
            <p class="paragraph-3 nerf_text nerf_results_text">
                We present <b style="color: rgb(228, 202, 115);">GenMM</b>, a generative model that "mines" as many diverse motions as possible from a single or 
                few example sequences. In stark contrast to existing data-driven methods, which typically require long 
                offline training time, are prone to visual artifacts, and tend to fail on large and complex skeletons, 
                GenMM inherits the training-free nature and the superior quality of the well-known Motion Matching method. 
                GenMM can synthesize a high-quality motion within a fraction of a second, even with highly complex and large 
                skeletal structures. At the heart of our generative framework lies the generative motion matching module, 
                which utilizes the bidirectional visual similarity as a generative cost function to motion matching,
                and operates in a multi-stage framework to progressively refine a random guess using exemplar motion matches. 
                In addition to diverse motion generation, we show the versatility of our generative framework by extending it 
                to a number of scenarios that are not possible with motion matching alone, including motion completion, key 
                frame-guided generation, infinite looping, and motion reassembly.
            </p>
        </div>
    </div>

    <div class="white_section_nerf grey_container w-container">
        <h2 class="grey-heading_nerf">BibTeX</h2>
        <div class="bibtex">
            <pre><code>@article{weiyu23GenMM,
    author    = {Weiyu Li and Xuelin Chen and Peizhuo Li and Olga Sorkine-Hornung and Baoquan Chen},
    title     = {Example-based Motion Synthesis via Generative Motion Matching},
    journal   = {ACM Transactions on Graphics (TOG)},
    year      = {2023},
    publisher = {ACM}
}</code></pre>
        </div>
    </div>

</body>
<footer>
    <!-- TBD -->
</footer>

</html>